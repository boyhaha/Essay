# 基础知识
[转自](https://zq99299.github.io/note-book/elasticsearch-core/)

## core

* 全文搜索, 倒排索引
* Lucene
* Elasticsearch
  * 自动维护数据的分布到多个节点的索引建立、检索请求分布到多个节点的执行
  * 自动维护数据的冗余副本，保证一些机器宕机了，不会丢失任何数据
  * 封装了更多的高级功能, 复杂搜索, 聚合分析全文检索，同义词处理，相关度排名，复杂数据分析，海量数据的近实时处理, 基于地理位置搜索
  * 支持PB级数据
  * 作为传统数据库的补充, 提供了数据库所不能提供的很多功能

## 入门

* 面向文档的数据格式

### 集群管理
1. 快速查看集群健康状况
   * ```
     GET /_cat/health?v
     参数v: 显示标题头
     unassign：未分配数量
     active_shards_percent：可用 shards 百分比
     ```
    * status 状态
      * green：每个索引的 primary shard 和 replica shard 都是 active 状态的
      * yellow：每个索引的 primary shard 都是 active 状态的，但是部分 replica shard 不是 active 状态，处于不可用的状态
      * red：不是所有索引的 primary shard 都是 active 状态的，部分索引有数据丢失了
2. 查看集群索引
   * ```
      GET /_cat/indices?v
      pri 默认是 5 个，rep 默认是 1 个
     ```
3. 查看节点使用情况
   * ```
      GET _cat/nodes?h=name,fm,fcm,sm,qcm,im&v

      fielddata.memory_size (fm), // 字段缓存占用内存
      filter_cache.memory_size (fcm) // 过滤语句缓存占用内存
      segments.memory (sm) // 每个分片包含的断 占用内存
     ```
4. 查看索引内存占用情况
   * ```
      GET _cat/segments?v
      GET _cat/segments?v&h=index,size,size.memory
     ```

### CRUD
1. 创建索引
  * ```
    PUT test_index
    {
      "mappings": {
        "properties" : {
          "create_time" : {
            "type":   "date",
            "format": "yyyy-MM-dd HH:mm:ss"
            },
          "gender" : {
            "type" : "long"
          },
          "id" : {
            "type" : "long"
          },
          "name" : {
            "type" : "text",
            "fields" : {
              "keyword" : {
                "type" : "keyword",
                "ignore_above" : 256
              }
            }
          }
        }
      }
    }
    ```
  * 设置ignore_above后，超过给定长度后的数据将不被索引，无法通过term精确匹配检索返回结果

2. 删除索引
   * ```
      DELETE /test_index?pretty
     ```
3. 新增数据
   * ```
      PUT /user_test/_doc/1
      {
        "create_time": "2020-05-30 12:26:25"
      }

      *** resp ***

      {
        "_index" : "test_index",
        "_type" : "_doc",
        "_id" : "4005",  # 数据版本号
        "_version" : 1,
        "result" : "created",
        "_shards" : {
          "total" : 2,  # 总的要写的分片数量
          "successful" : 2,
          "failed" : 0
        },
        "_seq_no" : 6,
        "_primary_term" : 1
      }
     ```

4. 查询数据
   * ```
      GET /user_test/_search
      {
        "query": {
          "match_all": {}
        }
      }
     ```
5. 替换数据
   * ```
      PUT /user_test/_doc/4005
      {
        "create_time": "2020-05-30 14:26:25"
      }
     ```
    * 替换_id 为4005的所有数据, 如果不存在, 则插入
    * 全量替换document内的json串内容
    * old_document 会标记为deleted, lazy deleted
    * 如果加上 _create, 则为强制创建, 如果存在, 则报错
6. 更新数据
   * ```
      POST /user_test/_update/2119
      {
        "doc": {
          "name": "update name"
        }
      }
     ```
    * partial update: 局部更新
      * 内置乐观锁并发控制
      * retry策略
        * 如果version版本冲突, 放弃更改
        * 基于最新版本号和数据再去更新
        * 重试次数为可以通过 retry 参数指定
    * 优点:
      * 查询, 修改, 写会操作都发生在es的一个shard 内部, 性能高
      * 减少了查询和修改中的间隔, 可有效减少并发冲突
    * upsert: 存在更新, 不存在插入
7. 删除文档
   * ```
      DELETE /user_test/2119
      {
        "doc": {
          "name": "update name"
        }
      }
     ```
    * lazy delete
      * 并不是直接删除, 而是标记为deleted, 当到达一定条件后es会自动删除
### query DSL(Domain Specified Language，特定领域的语言)
1. 查询所有
   * ```
      GET /user_test/_search
      {
        "query": {
          "match_all": {}
        }
      }
     ```
    * 响应
      ```
      {
        "took" : 2,
        "timed_out" : false,
        "_shards" : {
          "total" : 1,
          "successful" : 1,
          "skipped" : 0,
          "failed" : 0
        },
        "hits" : {
          "total" : {
            "value" : 10000,
            "relation" : "gte"
          },
          "max_score" : 1.0,
          "hits" : [
            {
              "_index" : "user_test",
              "_type" : "_doc",
              "_id" : "2219",
              "_score" : 1.0,
              "_source" : {
                "name" : "aaa",
                "gender" : 1,
                "state" : 1,
                "create_time" : "2015-02-05T18:29:50.068955",
                "id" : 2219
              }
            }
          ]
        }
      }
      ```
      * took: 查询时间
      * timed_out: 是否超时
      * _shards: 查询分片信息
      * hits.total: 查询结果数量
      * hits.max_score: 最高相关度
      * hits.hits: 匹配到的document list

2. 条件查询
   * 根据名称查询, 并且根据时间降序
   * ```
      GET user_test/_search
      {"query": {
            "bool": {
                "filter": {
                    "range": {
                        "create_time": {
                          "lt": "2020-07-10T12:24:00.00"
                        }
                    }
                }
            }
        },
        "sort": { "create_time": { "order": "desc" }},
        "size": 100
      }

     ```
3. 分页查询
   * from: 表示从第几条数据开始, 而不是页数
   * ```
      GET user_test/_search
      {"query": {
            "bool": {
                "filter": {
                    "range": {
                        "create_time": {
                          "lt": "2020-07-10T12:24:00.00"
                        }
                    }
                }
            }
        },
        "sort": { "create_time": { "order": "desc" }},
        "size": 1,
        "from": 2
      }

     ```
4. 限制返回字段
   * ```
      GET /user_test/_search
      {
        "query": {
          "match_all": {}
        },
        "_source": ["name","create_time"]
      }
     ```
5. query filter
   * ```
      GET /user_test/_search
      {
        "query": {
          "match_all": {}
        },
        "_source": ["name","create_time"]
      }
     ```
   * 查询方式
     1. match_all: 查询所有
     2. match: 对查询字段分词查询
     3. match_phrase(短语匹配): 完全包含查询字符串(分词之后)
        1. slop: n
           1. 少匹配几个也可以
        2. minimum_should_match: "100%"
           1. 最少匹配度
     4. multi_match(多字段查询)
        1. type:
           1. best_fields(最佳字段): 为每个字段生成一个match, 评分越高, 越靠前
              1. 通过best_fields策略，以及综合考虑其他field，还有minimum_should_match支持，可以尽可能精准地将匹配的结果推送到最前面
              2. 除了那些精准匹配的结果，其他差不多大的结果，排序结果不是太均匀，没有什么区分度
              3. eg: 百度搜索
           2. most_fields(多数字段): 匹配到的字段越多越好
              1. 将尽可能匹配更多field的结果推送到最前面，整个排序结果是比较均匀的
              2. 可能那些精准匹配的结果，无法推送到最前面
              3. eg: wiki
           3. cross_fields(混合字段): 类似将不同field合并为一个大字段, 然后完全包含查询词
6. full-text search
7. 短语搜索(match_phrase)
8. 高亮搜索结果
   * ```
      GET /user_test/_search
      {
          "query" : {
              "match_phrase" : {
                  "name" : "thanks"
              }
          },
          "highlight": {
            "fields": {
              "name": {}
            }
          }
      }
     ```


### 聚合分析
1. 语法
   ```
   GET /user_test/_search
    {
      "aggs": {
        "NAME": {
          "AGG_TYPE": {}
        }
      }
    }
   ```
   * aggs: 聚合函数
   * NAME: 命名
   * AGG_TYPE: 聚合类型: terms/ avg
2. 聚合
   * ```
      GET /user_test/_search
      {
        "aggs": {
          "group_by_gender": {
            "terms": {
              "field": "gender"
            }
          }
        },
        "size": 0
      }

      ----------------------------------------------------------
      响应
      {
        "took" : 164,
        "timed_out" : false,
        "_shards" : {
          "total" : 1,
          "successful" : 1,
          "skipped" : 0,
          "failed" : 0
        },
        "hits" : {
          "total" : {
            "value" : 10000,
            "relation" : "gte"
          },
          "max_score" : null,
          "hits" : [ ]
        },
        "aggregations" : {
          "group_by_gender" : {
            "doc_count_error_upper_bound" : 0,
            "sum_other_doc_count" : 0,
            "buckets" : [
              {
                "key" : 1,
                "doc_count" : 46630
              },
              {
                "key" : 2,
                "doc_count" : 40823
              },
              {
                "key" : 0,
                "doc_count" : 12547
              }
            ]
          }
        }
      }

     ```
    * size: 控制响应中hits.hits 中的数据
3. 先搜索, 再聚合
4. 嵌套聚合
5. 多次嵌套(下钻操作)


## es 集群剖析
1. 对复杂分布式机制的透明隐藏特性
   1. 分片机制
   2. 集群发现机制
   3. shard 负载均衡
      1. 自动分配shard
      2. 集群扩容, shard重新分配
      3. 增加减少节点是的数据 re balance
2. master节点
   1. 管理es集群元数据, 自动选举
   2. 节点对等: master 节点不承载所有的请求，所以不存在单节点瓶颈

## 分布式文档数据库
### document 数据路由原理
1. 什么是数据路由
   * 创建document时, 该doc存放与哪一个shard
2. 路由算法
   * shard = hash(routing) % number_of_primary_shards
   * routing = _id or custom routing value
     * 默认的routing为 _id
     * 手动指定 put /index/type/id?routing=user_id
     * 手动指定 routing value，可以保证某一类 document 一定被路由到一个 shard 上去， 那么在后续进行应用级别的负载均衡，以及提升批量读取的性能的时候，是很有帮助的
   * 因为路由算法的限制, 导致primary shard 数量不可变
3. 写一致性, quorum机制
   1. 发送增删改请求时可以带上一个 consistency 参数, 指明写一致性
      1. one: 只要有一个primary shard 是active
      2. all: 所有 primary shard 和 replica shard 都是活跃的
      3. quorum(default): 大部分shard都是活跃的
         1. 前提: number_of_replicas>1
         2. quorum = int( (primary + number_of_replicas) / 2 ) + 1
4. document 查询内部原理
   1. client 发送请求到任意node, 成为coordinate node(协调节点)
   2. 协调节点对document进行路由, 将请求转发到对应的node
      * 此时会使用round-robin随机轮询算法, 在 pri, rep 中选择一个, 进行负载均衡
   3. 接受请求的node返回document给协调节点
   4. 协调节点返回document给客户端

### 初识搜索引擎
1. timeout机制: 在指定的超时时长内返回结果, 这个结果可能不是所有结果
2. multi-index搜索模式
   1. /_search: 所有索引
   2. /index1,index2/_search: 在两个索引下搜索数据
   3. /*1,*2/_search: 按照通配符去匹配多个索引
3. 分页搜索时的deep paging
   *  分页搜索 10 条数据，在搜索深分页（比如 10000 条以后的数据）， 每个节点会返回 10000+ 条数据进行排序后再选中其中的 10 条数据返回
   *  协调节点会保存大量的数据, 还要进行排序, 既耗费网络带宽, 耗费内存, 还耗费CPU, 所以尽量避免
4. mapping
   1. 查看mapping
      * ```
          GET /user_test/_mapping
        ```
      * 可在插入数据前收到创建索引, 密码es自动设置的mapping不符合预期

5. 元数据
   1. _index, 类似于MySQL的一个库
   2. _type: 类似于MySQL的一半表, 7.0 之后已不支持多type, 默认都为_doc
   3. _id: document唯一标识
      1. 可手动指定, 也可自动生成
      2. 自动生成
         1. 长度20
         2. URL安全, 经过了base64编码的id, 可以放在URL中传递
         3. GUID方式, 分布式系统并行生成时不可能发生冲突
   4. _source: 定制返回结果字段
   5. _version: 数据版本号
   6. _all: 插入一条document, es会自动将多个field的值, 用字符串的方式串联起来(空格 分割), 同时建立索引. 如果搜索没有指定field, 会默认搜索_all field

6. groovy语法
   1. 内置脚本
   2. 外置脚本
      1. 语法内容存放在/config/scripts 目录下, 指定文件来获取脚本内容
      2. 外置脚本里面的语法和内置脚本不太相同, 内置中会把数据的json串当成字符串操作
7. 批量查询 mget
    * 获取不同索引下的数据
8. bulk 批量增删改
    1. bulk size
        1.  bulk请求会加载到内存里, 如果太大, 性能会下降, 需要尝试
        2.  一般从1000--5000 条数据开始, 大小一般在5-15MB之间
    2. 请求内容
       1. 特殊格式: 多个请求写多个json串, 每个json串里面不能存在换行, 每个json串之间必须有一个换行
       2. 标准json格式: 将json串放在一个数组中, es会对它解析为一个JSONArray对象, 整个数据会在内存中出现一份一模一样的拷贝, 一份json文本, 一份JSONArray对象
    3. 标准json格式会耗费更多的时间解析请求, 占用更多的内存, 频繁的垃圾回收, 导致性能下降
9. 精确匹配和全文搜索
    1. exact value: 搜索内容和值完全相同
    2. full text: 对搜索内容分词, 缩写, 大小写转换等等
       1. 缩写
       2. 格式转换: like, liked, likes
       3. 大小写转换
       4. 同义词
10. 倒排索引原理
    1. 基本原理: 将document进行分词, 然后将结果作为索引
    2. normalization(人性化)
       1. 配合分词器
       2. 对field 进行大小写, 单复数, 时态, 同义词等方面的处理
11. 分词器
    1. 切分词语, normalization
       1. character filter: 对文本分词前, 进行预处理, 例如: 过滤HTML标签
       2. tokenizer: 分词
       3. token filter: 大小写转换, 复数转换, 同义词, 时态转换
    2. 查看分词结果
       * ```
          GET index_name/_analyze
          {
            "field": "name",
            "text": "xxxxx"
          }
         ```
       * 如果字段类型是 exact value, 使用该api会保存
12. mapping
    1. mapping中会定义每个field的数据类型, 不同数据类型对应不同的搜索行为(exact value / full text)
    2. 核心数据类型
       1. string
       2. byte, short, integer, long
       3. float, double
       4. boolean
       5. date
    3. mapping操作
       *   获取mapping
       * ```
          GET /user_test/_mapping
          
         ```
       * 创建索引时指定mapping
       * ```
          PUT user_test
          {
            "mappings": {
                "properties" : {
                  "name" : {
                    "type" : "text",
                    "fields" : {
                      "keyword" : {
                        "type" : "keyword",  // 数据类型
                        "ignore_above" : 256,
                        "analyzer": "english"  // 分词器
                        "index": "analyzed"  // 索引类型
                      }
                    }
                  }
                }
              }
          }
         ```
       * 索引类型:
         * analyzed: 全文full text
         * not_analyzed: 精准匹配 exact value,
         * no: 不索引
           * 无法使用 _analyze 查看该字段的分词结果
       * 只能创建index时手动建立mapping, 或者新增field mapping, 但是不能update field mapping
    4. field:
       * multi value field 内数据类型不能混({ "tags": [ "tag1", "tag2" ]})
       * empty field: null, [], [null]
### mapping
Elasticsearch索引mapping的写入、查看与修改
https://blog.csdn.net/napoay/article/details/52012249
### fielddata
circuit_breaking_exception
https://blog.csdn.net/u014017121/article/details/70312784
https://www.elastic.co/guide/cn/elasticsearch/guide/current/_limiting_memory_usage.html

https://uzshare.com/view/822075
